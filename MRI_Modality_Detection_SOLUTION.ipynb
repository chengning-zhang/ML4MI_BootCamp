{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Copy of MRI_Modality_Detection_SOLUTION.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmjohnson3/ML4MI_Bootcamp_Development/blob/master/MRI_Modality_Detection_SOLUTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3602ceGz4oT",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "This tutorial will give an example application of using deep learning for categorization of images. This example will demonstrate how to implement a convolutional neural network for the identifying the type of MRI contrast or pulse sequence from a given input. The tutorial will have 3 main parts:\n",
        "\n",
        "1. Loading and organization of data for model training\n",
        "2. Creating a multi-class categorization deep learning model\n",
        "3. Training with pre-defined networks\n",
        "\n",
        "Keep an eye out for questions through this demo to test your new DL knowledge and critical thinking. There are answers at the end of the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPeOcqf5z4oV",
        "colab_type": "text"
      },
      "source": [
        "### Initial Preparation\n",
        "These are some modules that we will need throughout this example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPztvUIBz4oW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# initialize random seeds for reproducible results\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDDeiaCKz4oY",
        "colab_type": "text"
      },
      "source": [
        "We will import other necessary modules as we go and need them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-LkKRON0TpE",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to copy the files to a place where our CoLab notebook can read them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXOg0P5W0fUP",
        "colab_type": "code",
        "outputId": "1b8dec6c-6955-4d25-8622-86bc03a382ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Mount the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy and unarchive data\n",
        "!echo \"Copying Data Locally (Image Modality) -- Please be patient, this may take a while\"\n",
        "!tar xf /content/drive/'My Drive'/ML4MI_BOOTCAMP_DATA/ImageModalityDetector.tar --directory /home/\n",
        "!echo \"Complete!\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copying Data Locally (Image Modality)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1ScAI5iz4oZ",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Data Organization\n",
        "Data for this example has already been prepared for you. The data utilized here is from the 2017 MICCAI Multimodal Brain Tumor Segmentation (BRATS) Challenge. The data consists of magnetic resonance imaging (MRI) data from 19 different institutions in subjects with glioblastoma/high grade glioma (GBM/HGG) and low grade glioma (LGG). The different MRI scans include: T1-weighted (t1), post-contrast Gadolinium-enhanced images (t1ce), T2-weighted (t2), and T2 fluid attenuated inversion recovery (flair). More information on the 2017 BRATS Challenge is available here: https://www.med.upenn.edu/sbia/brats2017/data.html\n",
        "\n",
        "Data preparation for this example is similar to the approach that utilized in the previous example. This includes the use of a Keras image data generator. Your instructor has converted the BRATS data from the NiFTI imaging format (https://nifti.nimh.nih.gov/nifti-1/) to portable network graphics (.png) files, which is an image file format that can be read natively in Keras. Each .png file is a 256x256 axial image. In a later exercise, you will load image data directly from DICOM files as a further example. The structure of the data folder is as follows:\n",
        "<pre>\n",
        "  --brats_contrast_detector\n",
        "     --test (17,100 files)\n",
        "           --flair\n",
        "           --t1\n",
        "           --t1ce\n",
        "           --t2\n",
        "     --train (39,900 files)\n",
        "           --flair\n",
        "           --t1\n",
        "           --t1ce\n",
        "           --t2\n",
        "     --validate (17,100 files)\n",
        "           --flair\n",
        "           --t1\n",
        "           --t1ce\n",
        "           --t2\n",
        "</pre>\n",
        "\n",
        "Configuring the images in this particular folder structure allows easy use of a Keras ImageDataGenerator to perform the categorization of the different types of data for which we wish to discriminate. Specifically for this example, this includes the type of MRI scan: (flair, t1, t1ce, or t2) of each input image. The BRATS Dataset includes images from 285 subjects, and in this structure the data has been divided into ~66 subjects in the testing set (test), ~153 subjects in the training set (train), and ~66 subjects in the validation set (validate). Thus, each subject contributes 65 slices for each type of MRI scan.\n",
        "\n",
        "Now let's work on setting up the ImageDataGenerator to read in our data. Keras has built-in capabilities to perform basic types of data augmentation. That is the ability to apply different, randomized transformations to the input data to increase the diversity of the dataset. In this exercise we will explore the use of an ImageDataGenerator to perform varying types of augmentation. Let's try it out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRxXZPq4z4oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first let's use some varaibles to record information about our data\n",
        "dims = 256 # this is the size of out input data, 256x256\n",
        "#classes = ? # how many types of MRI scans do we have?\n",
        "#<SOLUTION>\n",
        "classes = 4\n",
        "#</SOLUTION>\n",
        "batch_size = 15 # this is how many input images will be utilized for each training step\n",
        "\n",
        "# this is the folder that contains the 39,900 images that will be used for training\n",
        "train_folder = '/home/ImageModalityDetector/train'\n",
        "# this is the folder htat contains the 17,100 images that will be used for validation\n",
        "valid_folder = '/home/ImageModalityDetector/validate'\n",
        "\n",
        "# set up the ImageDataGenerator for the validation data first. We will not perform augmentation on the validation data, but we do need to normalize the input intensity of the input images:\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# now set up the ImageDataGenerator for the training:\n",
        "# Go to the documentation page for the ImageDataGenerator (https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "#  and learn how to add in a rotation, a shear, width shift, height shift, and zoom augmentation.\n",
        "#  What are reasonable values for this type of dataset?\n",
        "#train_datagen = ?\n",
        "#<SOLUTION>\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,rotation_range=10,shear_range=1,width_shift_range=0.2,height_shift_range=0.2,zoom_range=0.2)\n",
        "#</SOLUTION>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaA-M3TLz4oc",
        "colab_type": "text"
      },
      "source": [
        "# Model Configuration\n",
        "In this example we will explore the use of different model structures that are already available in Keras. But first let's start with the model that used in the last example. Can you modify the existing model structure? How will it work with the number of classes in this new dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lnY8asqz4od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# insert the model you used in the last example here\n",
        "#<SOLUTION>\n",
        "img_input = layers.Input(shape=(dims,dims,1))\n",
        "x = layers.Conv2D(15, (3, 3), strides=(4,4), padding='same', kernel_initializer='he_normal')(img_input)\n",
        "x = layers.Activation('relu')(x)\n",
        "x = layers.MaxPooling2D((2, 2), strides=None)(x)\n",
        "x = layers.Flatten()(x)     #reshape to 1xN\n",
        "x = layers.Dense(20, activation='relu', kernel_initializer='he_normal')(x)\n",
        "x = layers.Dense(classes, activation='relu')(x)\n",
        "model = Model(inputs=img_input, outputs=x)\n",
        "#</SOLUTION>\n",
        "\n",
        "# now lets compile the model.\n",
        "#  What loss function should be used? binary_crossentropy does not make sense anymore.\n",
        "#    Here is a list of available loss functions in Keras: https://keras.io/losses/\n",
        "#  What metrics should be used? accuracy is also not appropriate here.\n",
        "#    Here is a list of available metrics in Keras: https://keras.io/metrics/\n",
        "#model.compile(loss=\"?\", optimizer=optimizers.Adam(lr=1e-3), metrics=[\"?\"])\n",
        "#<SOLUTION>\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=1e-5), metrics=[\"categorical_accuracy\"])\n",
        "#</SOLUTION>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amra_0kSz4of",
        "colab_type": "text"
      },
      "source": [
        "# Model Training\n",
        "Let's work on training this this model and dataset.\n",
        "\n",
        "First, let's set up the ImageDataGenerator to use the flow_from_directory function. This is very useful because it allows us to train large datasets that we might not be able to entirely load into our computer's memory. Also, it provides the facilities for on-the-fly augmentation that we are now using. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7sLnUjsz4og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4a569ff4-acec-4af6-b1a5-7aac477982cd"
      },
      "source": [
        "# the call to flow_from_directory, technically it returns a DirectoryIterator object\n",
        "#  that we pass to the model.fit_generator. Let's set it up:\n",
        "# What should class_mode be set to here?\n",
        "#train_generator = train_datagen.flow_from_directory(train_folder, batch_size=batch_size, target_size=(dims,dims), shuffle=True, class_mode='?', color_mode='grayscale')\n",
        "#valid_generator = valid_datagen.flow_from_directory(valid_folder, batch_size=batch_size, target_size=(dims,dims), shuffle=True, class_mode='?', color_mode='grayscale')\n",
        "#<SOLUTION>\n",
        "train_generator = train_datagen.flow_from_directory(train_folder, batch_size=batch_size, target_size=(dims,dims), shuffle=True, class_mode='categorical', color_mode='grayscale')\n",
        "valid_generator = valid_datagen.flow_from_directory(valid_folder, batch_size=batch_size, target_size=(dims,dims), shuffle=True, class_mode='categorical', color_mode='grayscale')\n",
        "#</SOLUTION>"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 39900 images belonging to 4 classes.\n",
            "Found 17100 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgZRoabfz4oi",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to start training. Let's get this get this going:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwKb5CEpz4oi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "a2cc56f7-4c35-45bb-a138-2befb4ef1f7a"
      },
      "source": [
        "# steps is the number of batches that are passed during each epoch\n",
        "#   IRL, this should be large enough such that we pass through the entire\n",
        "#        dataset (or equivalent) for each epoch. That is: 9975/batch_size\n",
        "#   For practicality for this course, let's make it smaller so we can progress\n",
        "#    through this exercise at a quicker pace.\n",
        "steps = 500\n",
        "# val_steps is the number of batches that are passed during each validation epoch\n",
        "#   IRL, this should be large enough such that we pass through the entire\n",
        "#        dataset (or equivalent) for each epoch. That is: 4275/batch_size\n",
        "#   For practicality for this course, let's make it smaller so we can progress\n",
        "#    through this exercise at a quicker pace.\n",
        "val_steps = 100\n",
        "\n",
        "# now let's train for 5 epochs (note that this is unrealistic demonstration of model training)\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps, epochs=5, \n",
        "                              validation_data=valid_generator, validation_steps=val_steps)\n",
        "\n",
        "# It should take approximately 5 minutes to train. Maybe you should take a break"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0819 21:58:57.217215 140367386662784 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "500/500 [==============================] - 56s 113ms/step - loss: 4.1248 - categorical_accuracy: 0.2556 - val_loss: 1.4838 - val_categorical_accuracy: 0.2453\n",
            "Epoch 2/5\n",
            "390/500 [======================>.......] - ETA: 11s - loss: 1.4618 - categorical_accuracy: 0.2516"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-443c5a94ce2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# now let's train for 5 epochs (note that this is unrealistic demonstration of model training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m history = model.fit_generator(train_generator, steps_per_epoch=steps, epochs=5, \n\u001b[0;32m---> 11\u001b[0;31m                               validation_data=valid_generator, validation_steps=val_steps)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# It should take approximately 5 minutes to train. Maybe you should take a break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoBooSO7z4ok",
        "colab_type": "text"
      },
      "source": [
        "Now that training is complete. Let's plot the losses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P78yHdQuz4ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "8f01eb81-8445-43d1-b10b-228064ce821d"
      },
      "source": [
        "# first, we need to import matplotlib to enable plotting\n",
        "#%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6.0, 4.0));\n",
        "# Plot the losses\n",
        "plt.subplot(121)\n",
        "plt.plot(history.epoch,history.history['loss'],'b-s')\n",
        "plt.plot(history.epoch,history.history['val_loss'],'r-s')\n",
        "plt.legend(['Training Data',\n",
        "            'Validation Data'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Loss Plot')\n",
        "# Plot the accuracy\n",
        "plt.subplot(122)\n",
        "plt.plot(history.epoch,history.history['categorical_accuracy'],'b-s')\n",
        "plt.plot(history.epoch,history.history['val_categorical_accuracy'],'r-s')\n",
        "plt.legend(['Training Data',\n",
        "            'Validation Data'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Plot')\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b5d9dd62ff03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot the losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b-s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r-s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m plt.legend(['Training Data',\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAAD8CAYAAADHTWCVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkBqjkJr/AxhqCwB1QOZxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1RQlckwz4on",
        "colab_type": "text"
      },
      "source": [
        "**Question 1:** Given the number of input classes that we have. What would be the accuracy of a model that was as good as a random guess? What was the accuracy of your model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3cP7H7Uz4oo",
        "colab_type": "text"
      },
      "source": [
        "### Improved Network Structures\n",
        "I hope you agree that we can do better. Let's try this again but with different model. Keras has the functionality to use pre-existing network structures. Let's explore that functionality. Please take a look at the Keras Applications page (https://keras.io/applications/), which describes pre-configured networks that are available.\n",
        "\n",
        "We will try MobileNet, which is designed to be an efficient network. Here is more information about the MobileNet structure: https://arxiv.org/pdf/1704.04861.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8txHv6kWz4op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "37107a8f-1a42-4655-ae4e-10f314c03e17"
      },
      "source": [
        "# first we need to import MobileNet\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "\n",
        "# here instantiate a MobileNet that is specific to our data (image size and number of classes) with randomized initial weights\n",
        "#model_mn = ?\n",
        "#<SOLUTION>\n",
        "model_mn = MobileNet(weights=None, input_shape=(dims,dims,1), classes=classes)\n",
        "#</SOLUTION>\n",
        "\n",
        "# now let's train it\n",
        "model_mn.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=1e-5), metrics=[\"categorical_accuracy\"])\n",
        "history_mn = model.fit_generator(train_generator, steps_per_epoch=steps, epochs=5,\n",
        "                              validation_data=valid_generator, validation_steps=val_steps)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 27/500 [>.............................] - ETA: 48s - loss: 1.4158 - categorical_accuracy: 0.2889"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b98e03542d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel_mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categorical_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m history_mn = model.fit_generator(train_generator, steps_per_epoch=steps, epochs=5,\n\u001b[0;32m---> 12\u001b[0;31m                               validation_data=valid_generator, validation_steps=val_steps)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y85nHYDDz4os",
        "colab_type": "text"
      },
      "source": [
        "Now we can plot the losses compared to our first network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQFBd4VUz4ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6.0, 4.0));\n",
        "plt.subplot(121)\n",
        "plt.plot(history_mn.epoch,history_mn.history['loss'],'b-s')\n",
        "plt.plot(history.epoch,history.history['loss'],'g-s')\n",
        "plt.legend(['MobileNet Training Data',\n",
        "            '1st Network Training Data'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.subplot(122)\n",
        "plt.plot(history_mn.epoch,history_mn.history['val_categorical_accuracy'],'r-s')\n",
        "plt.plot(history.epoch,history.history['val_categorical_accuracy'],'c-s')\n",
        "plt.legend(['MobileNet Validation Data',\n",
        "            '1st Network Validation Data'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Plot')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sate6Ep4z4ov",
        "colab_type": "text"
      },
      "source": [
        "This looks like it is doing much better. Unfortunately we do not have the time today to complete the full training. Fortunately, your instructor has trained this network (and several others) for up to 30 epochs. Let's work on next loading these trained models and compare their performance. Note that we are using a Keras callback function, ModelCheckpoint (https://keras.io/callbacks/#modelcheckpoint), to save the best weights of the trained network. We will load these weight files in the next step. \n",
        "\n",
        "For your information, the code that was used to train the different networks is below:\n",
        "\n",
        "#### To define VGG16 - https://arxiv.org/abs/1409.1556\n",
        "<pre>\n",
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16(weights=None, input_shape=(dims,dims,1), classes=classes)\n",
        "</pre>\n",
        "#### To define InceptionV3 - http://arxiv.org/abs/1512.00567\n",
        "<pre>\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "model = InceptionV3(weights=None, input_shape=(dims,dims,1), classes=classes)\n",
        "</pre>\n",
        "#### To define DenseNet - https://arxiv.org/abs/1608.06993\n",
        "<pre>\n",
        "from keras.applications.densenet import DenseNet121\n",
        "model = DenseNet121(weights=None, input_shape=(dims,dims,1), classes=classes)\n",
        "</pre>\n",
        "#### Used to train all of the above models\n",
        "<pre>\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=1e-4), metrics=[\"categorical_accuracy\"])\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint('weights.h5', monitor='loss', save_best_only=True)\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=steps, epochs=30, callbacks=[model_checkpoint],\n",
        "                              validation_data=valid_generator, validation_steps=val_steps )\n",
        "</pre>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DN9YUB7z4ov",
        "colab_type": "text"
      },
      "source": [
        "### Comparing Different Models\n",
        "Let's start by loading up the different history files that were saved for each model, and let's take a look at the loss plot for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUDpuAJcz4ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load in history files\n",
        "history_inceptionv3 = numpy.load('history_inceptionv3.npy')\n",
        "history_mobilenet = numpy.load('history_mobilenet.npy')\n",
        "history_vgg16 = numpy.load('history_vgg16.npy')\n",
        "\n",
        "# let's plot them\n",
        "plt.figure(figsize=(6.0, 4.0));\n",
        "plt.subplot(121)\n",
        "plt.plot(history_inceptionv3.all()['loss'],'b-s')\n",
        "plt.plot(history_mobilenet.all()['loss'],'r-s')\n",
        "plt.plot(history_vgg16.all()['loss'],'g-s')\n",
        "plt.legend(['InceptionV3',\n",
        "            'MobileNet',\n",
        "            'VGG16'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylabel('Loss Plot')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(history_inceptionv3.all()['categorical_accuracy'],'b-s')\n",
        "plt.plot(history_mobilenet.all()['categorical_accuracy'],'r-s')\n",
        "plt.plot(history_vgg16.all()['categorical_accuracy'],'g-s')\n",
        "plt.legend(['InceptionV3',\n",
        "            'MobileNet',\n",
        "            'VGG16'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Plot')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMnvFg4_z4oy",
        "colab_type": "text"
      },
      "source": [
        "**Question 2:** Which network performs the best? Do you think training is complete after 30 epochs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb89I7t_z4oz",
        "colab_type": "text"
      },
      "source": [
        "Now let's evaluate some data and see what happens. Let's using the InceptionV3 model and load the existing trained weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT8jmTkNz4o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "model_i = InceptionV3(weights='weights_inceptionv3.h5', input_shape=(dims,dims,1), classes=classes)\n",
        "model_i.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=1e-4), metrics=[\"categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1WJu_-zz4o2",
        "colab_type": "text"
      },
      "source": [
        "Now let's work on testing the model with some data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0mV6dQRz4o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's set up an ImageDataGenerator for the test data\n",
        "test_folder = os.path.join(os.getcwd(),'test')\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(valid_folder, batch_size=1, target_size=(dims,dims), class_mode='categorical', color_mode='grayscale')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsKuIDy7z4o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the next image from the generator\n",
        "X,Y = test_generator.next()\n",
        "\n",
        "# visualize the current image\n",
        "plt.figure(figsize=(6.0, 4.0))\n",
        "plt.imshow(X[0,:,:,0],cmap='gray')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# now predict\n",
        "y = model_i.predict(X)\n",
        "\n",
        "# display the prediction as a printed text message\n",
        "actual_type = [key for key in test_generator.class_indices.items() if key[1] == numpy.argmax(Y)][0][0]\n",
        "predicted_type = [key for key in test_generator.class_indices.items() if key[1] == numpy.argmax(y)][0][0]\n",
        "print('The actual type was {}, the predicted type was {}'.format(actual_type,predicted_type))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySIrFM7Hz4o7",
        "colab_type": "text"
      },
      "source": [
        "You can re-run the cell above to get the next iteration from the test data generator. Keep running it until you identify several misclassifications of the MRI pulse sequence type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSBuxulHz4o7",
        "colab_type": "text"
      },
      "source": [
        "**Question 3:** When the network failed to identify the pulse sequence type, what characteristics did you notice in the images? For example, what regions of the brain did the algorithm seem to struggle with identifying? Do you think that you could have done better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW4rcjkKz4o8",
        "colab_type": "text"
      },
      "source": [
        "### End of Exercise\n",
        "This is the end of this exercise. If you have extra time and would like to try a few advanced ideas, please consider trying the following:\n",
        "1. Modify the code above to include predictions for MobileNet and VGG16. Perform the analysis on the 3 different networks for each slice. Are failure cases similar between networks?\n",
        "2. Try re-training the MobileNet network without augmentation. Do you notice any differences?\n",
        "3. Try adding different types of augmentation, such as left-right and up-down flips. What about adding more extreme degrees of augmentation? Does this improve network performance?\n",
        "4. Try adding augmentation to the evaluation stage to see how the network performs with augmentation.\n",
        "5. Explore the other types of available Callback functions during model fitting. There are many useful predefined types of callbacks that you can use to get more information from your network during training. Read more about them here: https://keras.io/callbacks/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4VdCdj4z4o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}